---
title: "大模型模型架构--Dense和MoE"
date: 2026-02-27T20:55:11+08:00
draft: false

categories:
- study

tags:
- LLM
---

Dense: 密集连接模型，每个神经元与前一层的所有神经元都有连接。
MoE(Mixture of Experts): 混合专家模型，基本特征是稀疏激活的使用。对任何输入，只有模型总参数的一小部分（少数几个"专家"子网络）会被激活并参与计算。

## Dense 
以GPT4为代表，架构核心特点：    
* **结构简洁**：模型内部连接密集，计算流程清晰直观
* **训练稳定**：全激活模式下梯度传播路径确定，优化过程相对稳定
* **部署成熟**：硬件加速技术（如CUDA、TPU）对Dense矩阵计算支持完善
* **推理延迟低**：单一计算路径使延迟较为稳定，适合实时交互场景

优点：
* 易用的训练和推理框架（TensorFlow、PyTorch）
* 充分理解的优化技术（反向传播、梯度下降）
* 强大的硬件支持（主流芯片厂商提供的加速方案）
* 推理过程表现出高度确定性和稳定性

缺点：
* 模型参数数量庞大，占用内存和计算资源
* 训练成本高，需要大量的计算和数据

商业应用（由于其可预测的推理延迟（特别是针对特定硬件优化后））：
* 实时交互应用：虚拟现实（VR）、增强现实（AR）、物联网设备
* 移动设备部署：通过模型压缩和优化技术（剪枝、量化），在资源有限的移动设备上实现高效推理
* 高频决策系统：量化交易、金融决策系统等需要极快响应的场景
* 监管严格行业：金融、医疗、法律等对模型输出可追溯性和解释性有强要求的领域


## MoE
架构核心特点：
* 专家模型的选择和训练
* 门控机制
* 专家模型的组合和输出
* 优化与训练策略，专家间的协同和竞争

优点：
* 容量提升，计算成本可控
* 广泛处理不同任务，无需为每个任务单独训练模型

缺点：
* 训练过程更复杂，需要协调门控网络学习和多个专家学习，确保路由机制正确分配输入，每个专家适当专业化而不过度专注或使用不足。专家间实现均衡工作负载分配是MoE训练中持续挑战。
* 推理过程内存占用增加
* 需要专门的ai硬件，支持稀疏计算

# Hybrid MoE
架构核心特点：
* 结合Dense和MoE的优势，在保持模型稀疏激活的同时，利用Dense模型的并行计算能力，提升推理效率。



